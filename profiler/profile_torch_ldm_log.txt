Not using distributed mode
[10:09:31.585092] job dir: /workspace/home/AtlasGaussians
[10:09:31.585120] Namespace(batch_size=27,
epochs=1000,
replica=8,
accum_iter=1,
model='kl_d512_m512_l16_d24_edm',
config_ae='config/shapenet/train_car_full.yaml',
ae_pth='output/vae/shapenet/vae_car_full/ckpt/checkpoint-999.pth',
clip_grad=None,
weight_decay=0.05,
lr=0.0001,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=50,
log_dir='output/ldm/shapenet/car',
latent_dir='output/vae/shapenet/vae_car_full/inference/latents/epoch_1000',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
output_dir='output/ldm/shapenet/car/ckpt',
distributed=False)
[10:09:31.598697] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f75a74b67b0>
[10:09:31.603267] Copied README.md to output/ldm/shapenet/car/code/README.md
[10:09:31.603322] Copied datasets/splits/shapenet/02691156_test.txt to output/ldm/shapenet/car/code/datasets/splits/shapenet/02691156_test.txt
[10:09:31.603367] Copied datasets/splits/shapenet/02691156_train.txt to output/ldm/shapenet/car/code/datasets/splits/shapenet/02691156_train.txt
[10:09:31.603408] Copied engine_ae.py to output/ldm/shapenet/car/code/engine_ae.py
[10:09:31.603441] Copied gs.py to output/ldm/shapenet/car/code/gs.py
[10:09:31.603475] Copied main_ae_profile_nvtx.py to output/ldm/shapenet/car/code/main_ae_profile_nvtx.py
[10:09:31.603511] Copied main_ae_profile_torch.py to output/ldm/shapenet/car/code/main_ae_profile_torch.py
[10:09:31.603543] Copied main_class_cond_profile_nvtx.py to output/ldm/shapenet/car/code/main_class_cond_profile_nvtx.py
[10:09:31.603575] Copied main_class_cond_profile_torch.py to output/ldm/shapenet/car/code/main_class_cond_profile_torch.py
[10:09:31.603606] Copied profiler/profile_nvtx_infer_log.txt to output/ldm/shapenet/car/code/profiler/profile_nvtx_infer_log.txt
[10:09:31.603636] Copied profiler/profile_nvtx_ldm_log.txt to output/ldm/shapenet/car/code/profiler/profile_nvtx_ldm_log.txt
[10:09:31.603674] Copied profiler/profile_nvtx_vae_log.txt to output/ldm/shapenet/car/code/profiler/profile_nvtx_vae_log.txt
[10:09:31.603703] Copied profiler/profile_torch_ldm_log.txt to output/ldm/shapenet/car/code/profiler/profile_torch_ldm_log.txt
[10:09:31.603743] Copied profiler/profile_torch_vae_log.txt to output/ldm/shapenet/car/code/profiler/profile_torch_vae_log.txt
[10:09:31.603773] Copied requirements.txt to output/ldm/shapenet/car/code/requirements.txt
[10:09:31.603803] Copied sample_class_cond_cfg_profile_nvtx.py to output/ldm/shapenet/car/code/sample_class_cond_cfg_profile_nvtx.py
[10:09:31.603835] Copied sample_class_cond_cfg_profile_torch.py to output/ldm/shapenet/car/code/sample_class_cond_cfg_profile_torch.py
[10:09:31.603865] Copied scripts/profile_ncu.sh to output/ldm/shapenet/car/code/scripts/profile_ncu.sh
[10:09:31.603892] Copied scripts/profile_nvtx.sh to output/ldm/shapenet/car/code/scripts/profile_nvtx.sh
[10:09:31.603921] Copied scripts/profile_torch.sh to output/ldm/shapenet/car/code/scripts/profile_torch.sh
[10:09:31.603948] Copied scripts/train_shape_chair.sh to output/ldm/shapenet/car/code/scripts/train_shape_chair.sh
[10:09:31.603976] Copied scripts/train_shape_plane.sh to output/ldm/shapenet/car/code/scripts/train_shape_plane.sh
[10:09:33.312933] Loading autoencoder output/vae/shapenet/vae_car_full/ckpt/checkpoint-999.pth
[10:09:35.263230] Model = EDMPrecond(
  (model): LatentArrayTransformer(
    (proj_in): Linear(in_features=16, out_features=512, bias=False)
    (transformer_blocks): ModuleList(
      (0-23): 24 x BasicTransformerBlock(
        (attn1): CrossAttention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ff): FeedForward(
          (net): Sequential(
            (0): GEGLU(
              (proj): Linear(in_features=512, out_features=4096, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (attn2): CrossAttention(
          (to_q): Linear(in_features=512, out_features=512, bias=False)
          (to_k): Linear(in_features=512, out_features=512, bias=False)
          (to_v): Linear(in_features=512, out_features=512, bias=False)
          (to_out): Sequential(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm1): AdaLayerNorm(
          (silu): SiLU()
          (linear): Linear(in_features=512, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
        (norm2): AdaLayerNorm(
          (silu): SiLU()
          (linear): Linear(in_features=512, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
        (norm3): AdaLayerNorm(
          (silu): SiLU()
          (linear): Linear(in_features=512, out_features=1024, bias=True)
          (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (ls2): Identity()
        (drop_path2): Identity()
        (ls3): Identity()
        (drop_path3): Identity()
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj_out): Linear(in_features=512, out_features=16, bias=False)
    (map_noise): PositionalEmbedding()
    (map_layer0): Linear(in_features=256, out_features=512, bias=True)
    (map_layer1): Linear(in_features=512, out_features=512, bias=True)
  )
)
[10:09:35.263247] number of params (M): 164.20
[10:09:35.263252] actual lr: 1.00e-04
[10:09:35.263254] accumulate grad iterations: 1
[10:09:35.263256] effective batch size: 27
[10:09:35.263910] criterion = <models_class_cond.EDMLoss object at 0x7f7384405820>
[10:09:35.263916] Start training for 1000 epochs
[10:09:35.278897] log_dir: output/ldm/shapenet/car
[10:09:35.940219] Epoch: [0]  [0/2]  eta: 0:00:01  lr: 0.000000  loss: 0.9523 (0.9523)  time: 0.6030  data: 0.1186  max mem: 27316
[10:09:36.267855] Epoch: [0]  [1/2]  eta: 0:00:00  lr: 0.000001  loss: 0.9507 (0.9515)  time: 0.4651  data: 0.0593  max mem: 28539
[10:09:36.291682] Epoch: [0] Total time: 0:00:00 (0.4776 s / it)
[10:09:36.291721] Averaged stats: lr: 0.000001  loss: 0.9507 (0.9515)
[10:09:37.813322] Test:  [0/2]  eta: 0:00:00  loss: 0.9954 (0.9954)  time: 0.1426  data: 0.1127  max mem: 28539
[10:09:37.831671] Test:  [1/2]  eta: 0:00:00  loss: 0.9954 (0.9969)  time: 0.0803  data: 0.0564  max mem: 28539
[10:09:37.854494] Test: Total time: 0:00:00 (0.0921 s / it)
[10:09:37.854555] * loss 0.997
[10:09:37.854676] loss of the network on the 2 test images: 0.997
[10:09:37.860401] log_dir: output/ldm/shapenet/car
[10:09:38.368148] Epoch: [1]  [0/2]  eta: 0:00:01  lr: 0.000002  loss: 0.9495 (0.9495)  time: 0.5018  data: 0.1413  max mem: 28540
[10:09:38.743694] Epoch: [1]  [1/2]  eta: 0:00:00  lr: 0.000003  loss: 0.9495 (0.9615)  time: 0.4384  data: 0.0707  max mem: 28540
[10:09:38.765798] Epoch: [1] Total time: 0:00:00 (0.4499 s / it)
[10:09:38.765834] Averaged stats: lr: 0.000003  loss: 0.9495 (0.9615)
[10:09:38.772200] log_dir: output/ldm/shapenet/car
[10:09:39.275381] Epoch: [2]  [0/2]  eta: 0:00:00  lr: 0.000004  loss: 0.9382 (0.9382)  time: 0.4973  data: 0.1350  max mem: 28540
[10:09:39.652075] Epoch: [2]  [1/2]  eta: 0:00:00  lr: 0.000005  loss: 0.9382 (0.9412)  time: 0.4368  data: 0.0675  max mem: 28540
[10:09:39.675381] Epoch: [2] Total time: 0:00:00 (0.4489 s / it)
[10:09:39.675422] Averaged stats: lr: 0.000005  loss: 0.9382 (0.9412)
[10:09:57.933885] -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                          train_epoch_0         0.00%       0.000us         0.00%       0.000us       0.000us     987.701ms        46.59%     987.701ms     987.701ms           0 b           0 b           0 b           0 b             1  
                                          train_epoch_1         0.00%       0.000us         0.00%       0.000us       0.000us     882.381ms        41.62%     882.381ms     882.381ms           0 b           0 b           0 b           0 b             1  
                                          train_epoch_2         0.00%       0.000us         0.00%       0.000us       0.000us     878.987ms        41.46%     878.987ms     878.987ms           0 b           0 b           0 b           0 b             1  
                                               aten::mm         0.74%      28.748ms         7.99%     312.052ms      66.907us     820.824ms        38.71%     822.559ms     176.363us           0 b           0 b      61.85 Gb      61.85 Gb          4664  
    autograd::engine::evaluate_function: AddmmBackward0         0.18%       7.007ms        10.13%     395.877ms     388.114us       0.000us         0.00%     582.707ms     571.281us           0 b           0 b     -27.57 Gb     -57.16 Gb          1020  
                                         AddmmBackward0         0.08%       3.199ms         6.79%     265.342ms     260.140us       0.000us         0.00%     550.837ms     540.036us           0 b           0 b      29.58 Gb           0 b          1020  
                                           aten::linear         0.11%       4.405ms         3.44%     134.575ms      49.476us       0.000us         0.00%     438.364ms     161.163us           0 b           0 b      58.35 Gb           0 b          2720  
                                            aten::addmm         1.79%      69.872ms         2.41%      94.059ms      60.605us     328.803ms        15.51%     341.598ms     220.102us           0 b           0 b      42.52 Gb      42.52 Gb          1552  
                                          train_epoch_0         6.20%     242.245ms        26.13%        1.021s        1.021s       0.000us         0.00%     289.531ms     289.531ms       1.91 Kb      -1.69 Mb       1.24 Gb     -84.38 Gb             1  
                                              aten::mul         0.32%      12.679ms         1.77%      69.076ms      19.167us     266.803ms        12.58%     267.029ms      74.092us         -32 b         -32 b     183.99 Gb     183.88 Gb          3604  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 3.907s
Self CUDA time total: 2.120s

[10:09:57.933909] Training time 0:00:22
