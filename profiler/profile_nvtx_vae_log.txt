Not using distributed mode
[23:22:46.573106] job dir: /home/rb/AtlasGaussians
[23:22:46.573128] Namespace(config_path='config/shapenet/train_car_full.yaml',
log_dir='output/vae/shapenet/vae_car_full',
resume='output/vae/shapenet/vae_car_full/ckpt/checkpoint-199_model_only.pth',
start_epoch=0,
eval=False,
infer=False,
dist_eval=False,
device='cuda',
distributed=False,
world_size=1,
local_rank=-1,
dist_url='env://',
output_dir='output/vae/shapenet/vae_car_full/ckpt')
[23:22:46.609550] ['02958343']
[23:22:46.609633] ['02958343']
[23:22:46.639427] Copied datasets/splits/shapenet/02691156_test.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02691156_test.txt
[23:22:46.639516] Copied datasets/splits/shapenet/02691156_train.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02691156_train.txt
[23:22:46.639600] Copied models_class_cond.py to output/vae/shapenet/vae_car_full/code/models_class_cond.py
[23:22:46.639668] Copied profiler/profile_nvtx_infer_log.txt to output/vae/shapenet/vae_car_full/code/profiler/profile_nvtx_infer_log.txt
[23:22:46.639724] Copied profiler/profile_nvtx_ldm_log.txt to output/vae/shapenet/vae_car_full/code/profiler/profile_nvtx_ldm_log.txt
[23:22:46.639753] Copied profiler/profile_nvtx_vae_log.txt to output/vae/shapenet/vae_car_full/code/profiler/profile_nvtx_vae_log.txt
[23:22:46.639787] Copied scripts/train_shape_car.sh to output/vae/shapenet/vae_car_full/code/scripts/train_shape_car.sh
[23:22:46.639814] Copied scripts/train_shape_plane.sh to output/vae/shapenet/vae_car_full/code/scripts/train_shape_plane.sh
[23:22:46.639859] Copied util/emd/emd_cuda.cu to output/vae/shapenet/vae_car_full/code/util/emd/emd_cuda.cu
[23:22:48.594095] Model = KLAutoEncoder(
  (img_encoder): LEAP_Encoder(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x NestedTensorBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (backbone_out): BackboneOutBlock()
  )
  (point_embed): PointFourierEncoder(
    (mlp): Linear(in_features=51, out_features=512, bias=True)
  )
  (pc_encoder): TransformerDecoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (multihead_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (linear1): Linear(in_features=512, out_features=2048, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (linear2): Linear(in_features=2048, out_features=512, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
  )
  (embed_layers): ModuleList(
    (0): TransformerDecoderLayerAda(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
    )
  )
  (enc_layers): Sequential(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
  )
  (mean_fc): Linear(in_features=512, out_features=16, bias=True)
  (logvar_fc): Linear(in_features=512, out_features=16, bias=True)
  (proj): Linear(in_features=16, out_features=512, bias=True)
  (latent_net): LatentNet(
    (ca_layer): TransformerDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
    )
    (layers): ModuleList(
      (0-7): 8 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (up_fc): Linear(in_features=512, out_features=1024, bias=True)
    (up_layers): ModuleList(
      (0-3): 4 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (lp_net): LPNet(
    (point_fourier_encoder): PointFourierEncoder(
      (mlp): Linear(in_features=34, out_features=128, bias=True)
    )
    (lps_net): LPSNet(
      (up_fc): Linear(in_features=256, out_features=512, bias=True)
      (global_fc): Linear(in_features=256, out_features=128, bias=True)
      (anchor_fc): Linear(in_features=256, out_features=128, bias=True)
      (geom_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (attr_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (global_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (anchor_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (to_anchor): Sequential(
        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): GELU(approximate='none')
        (3): Linear(in_features=128, out_features=3, bias=True)
      )
    )
    (gs_decoder_geom): GS_Decoder(
      (decoder_cross_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (to_outputs): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): GELU(approximate='none')
      )
      (to_pos): Linear(in_features=128, out_features=3, bias=True)
      (to_opacity): Linear(in_features=128, out_features=1, bias=True)
      (to_scale): Linear(in_features=128, out_features=3, bias=True)
      (to_rotation): Linear(in_features=128, out_features=4, bias=True)
      (to_rgb): Linear(in_features=128, out_features=3, bias=True)
    )
    (gs_decoder_attr): GS_Decoder(
      (decoder_cross_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (to_outputs): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): GELU(approximate='none')
      )
      (to_pos): Linear(in_features=128, out_features=3, bias=True)
      (to_opacity): Linear(in_features=128, out_features=1, bias=True)
      (to_scale): Linear(in_features=128, out_features=3, bias=True)
      (to_rotation): Linear(in_features=128, out_features=4, bias=True)
      (to_rgb): Linear(in_features=128, out_features=3, bias=True)
    )
  )
  (lpips_loss): LPIPS(
    (net): vgg16(
      (slice1): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (slice2): Sequential(
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (6): ReLU(inplace=True)
        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (8): ReLU(inplace=True)
      )
      (slice3): Sequential(
        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU(inplace=True)
        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (13): ReLU(inplace=True)
        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (15): ReLU(inplace=True)
      )
      (slice4): Sequential(
        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (18): ReLU(inplace=True)
        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (20): ReLU(inplace=True)
        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (22): ReLU(inplace=True)
      )
      (slice5): Sequential(
        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (25): ReLU(inplace=True)
        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (27): ReLU(inplace=True)
        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (29): ReLU(inplace=True)
      )
    )
    (scaling_layer): ScalingLayer()
    (lin0): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin1): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin2): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin3): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin4): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lins): ModuleList(
      (0): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3-4): 2 x NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
  )
)
[23:22:48.594124] number of params (M): 141.69
[23:22:48.594193] actual lr: 1.00e-04
[23:22:48.594204] accumulate grad iterations: 1
[23:22:48.594205] effective batch size: 4
[23:22:48.595278] criterion = BCEWithLogitsLoss()
[23:22:48.743856] Resume checkpoint output/vae/shapenet/vae_car_full/ckpt/checkpoint-199_model_only.pth
[23:22:48.744261] Start training for 1000 epochs
[23:22:48.745546] log_dir: output/vae/shapenet/vae_car_full
[23:22:52.144960] Epoch: [0]  [0/2]  eta: 0:00:06  lr: 0.000000  loss_kl: 0.1871 (0.1871)  loss_lp0: 0.0021 (0.0021)  loss_lp1: 0.0019 (0.0019)  loss_lp2: 0.0017 (0.0017)  loss_lp_render: 0.0017 (0.0017)  loss_lp_emd0: 0.0149 (0.0149)  loss_lp_emd2: 0.0097 (0.0097)  loss_lp_emd_render: 0.0091 (0.0091)  loss_render_rgb: 0.0090 (0.0090)  loss_render_depth: 0.2045 (0.2045)  loss_render_mask: 0.2057 (0.2057)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2469 (0.2469)  loss: 0.7072 (0.7072)  psnr: 20.7190 (20.7190)  time: 3.3897  data: 1.1061  max mem: 17342
[23:22:52.466098] Epoch: [0]  [1/2]  eta: 0:00:01  lr: 0.000001  loss_kl: 0.1871 (0.1916)  loss_lp0: 0.0021 (0.0022)  loss_lp1: 0.0019 (0.0019)  loss_lp2: 0.0017 (0.0018)  loss_lp_render: 0.0017 (0.0018)  loss_lp_emd0: 0.0149 (0.0154)  loss_lp_emd2: 0.0097 (0.0105)  loss_lp_emd_render: 0.0091 (0.0103)  loss_render_rgb: 0.0083 (0.0086)  loss_render_depth: 0.2045 (0.2079)  loss_render_mask: 0.2057 (0.2091)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2417 (0.2443)  loss: 0.7072 (0.7140)  psnr: 20.7190 (20.8377)  time: 1.8551  data: 0.5539  max mem: 17342
[23:22:52.503025] Epoch: [0] Total time: 0:00:03 (1.8787 s / it)
[23:22:52.506482] Averaged stats: lr: 0.000001  loss_kl: 0.1871 (0.1916)  loss_lp0: 0.0021 (0.0022)  loss_lp1: 0.0019 (0.0019)  loss_lp2: 0.0017 (0.0018)  loss_lp_render: 0.0017 (0.0018)  loss_lp_emd0: 0.0149 (0.0154)  loss_lp_emd2: 0.0097 (0.0105)  loss_lp_emd_render: 0.0091 (0.0103)  loss_render_rgb: 0.0083 (0.0086)  loss_render_depth: 0.2045 (0.2079)  loss_render_mask: 0.2057 (0.2091)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2417 (0.2443)  loss: 0.7072 (0.7140)  psnr: 20.7190 (20.8377)
[23:22:56.104381] Test:  [0/1]  eta: 0:00:02  loss_kl: 0.1849 (0.1849)  loss_lp0: 0.0021 (0.0021)  loss_lp1: 0.0019 (0.0019)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0161 (0.0161)  loss_lp_emd2: 0.0117 (0.0117)  loss_lp_emd_render: 0.0123 (0.0123)  loss_render_rgb: 0.0071 (0.0071)  loss_render_depth: 0.1843 (0.1843)  loss_render_mask: 0.1857 (0.1857)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2277 (0.2277)  loss: 0.6524 (0.6524)  psnr: 21.6193 (21.6193)  time: 2.4088  data: 1.6300  max mem: 17342
[23:22:56.173156] Test: Total time: 0:00:02 (2.4780 s / it)
[23:22:56.210031] log_dir: output/vae/shapenet/vae_car_full
[23:22:57.984298] Epoch: [1]  [0/2]  eta: 0:00:03  lr: 0.000002  loss_kl: 0.1839 (0.1839)  loss_lp0: 0.0022 (0.0022)  loss_lp1: 0.0019 (0.0019)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0148 (0.0148)  loss_lp_emd2: 0.0099 (0.0099)  loss_lp_emd_render: 0.0097 (0.0097)  loss_render_rgb: 0.0082 (0.0082)  loss_render_depth: 0.1963 (0.1963)  loss_render_mask: 0.1975 (0.1975)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2382 (0.2382)  loss: 0.6822 (0.6822)  psnr: 21.1109 (21.1109)  time: 1.7737  data: 1.4482  max mem: 17342
[23:22:58.301608] Epoch: [1]  [1/2]  eta: 0:00:01  lr: 0.000003  loss_kl: 0.1839 (0.1866)  loss_lp0: 0.0022 (0.0022)  loss_lp1: 0.0019 (0.0020)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0148 (0.0155)  loss_lp_emd2: 0.0099 (0.0106)  loss_lp_emd_render: 0.0097 (0.0105)  loss_render_rgb: 0.0082 (0.0085)  loss_render_depth: 0.1963 (0.2056)  loss_render_mask: 0.1975 (0.2066)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2382 (0.2405)  loss: 0.6822 (0.7056)  psnr: 20.8305 (20.9707)  time: 1.0453  data: 0.7241  max mem: 17342
[23:22:58.379703] Epoch: [1] Total time: 0:00:02 (1.0848 s / it)
[23:22:58.385036] Averaged stats: lr: 0.000003  loss_kl: 0.1839 (0.1866)  loss_lp0: 0.0022 (0.0022)  loss_lp1: 0.0019 (0.0020)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0148 (0.0155)  loss_lp_emd2: 0.0099 (0.0106)  loss_lp_emd_render: 0.0097 (0.0105)  loss_render_rgb: 0.0082 (0.0085)  loss_render_depth: 0.1963 (0.2056)  loss_render_mask: 0.1975 (0.2066)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2382 (0.2405)  loss: 0.6822 (0.7056)  psnr: 20.8305 (20.9707)
[23:22:58.388405] log_dir: output/vae/shapenet/vae_car_full
[23:23:00.238742] Epoch: [2]  [0/2]  eta: 0:00:03  lr: 0.000004  loss_kl: 0.1859 (0.1859)  loss_lp0: 0.0023 (0.0023)  loss_lp1: 0.0020 (0.0020)  loss_lp2: 0.0019 (0.0019)  loss_lp_render: 0.0019 (0.0019)  loss_lp_emd0: 0.0147 (0.0147)  loss_lp_emd2: 0.0116 (0.0116)  loss_lp_emd_render: 0.0111 (0.0111)  loss_render_rgb: 0.0093 (0.0093)  loss_render_depth: 0.2132 (0.2132)  loss_render_mask: 0.2144 (0.2144)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2430 (0.2430)  loss: 0.7253 (0.7253)  psnr: 20.5562 (20.5562)  time: 1.8497  data: 1.5319  max mem: 17342
[23:23:00.594925] Epoch: [2]  [1/2]  eta: 0:00:01  lr: 0.000005  loss_kl: 0.1786 (0.1823)  loss_lp0: 0.0022 (0.0022)  loss_lp1: 0.0020 (0.0020)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0147 (0.0152)  loss_lp_emd2: 0.0106 (0.0111)  loss_lp_emd_render: 0.0107 (0.0109)  loss_render_rgb: 0.0073 (0.0083)  loss_render_depth: 0.1937 (0.2035)  loss_render_mask: 0.1949 (0.2046)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2308 (0.2369)  loss: 0.6714 (0.6983)  psnr: 20.5562 (21.0435)  time: 1.1027  data: 0.7865  max mem: 17342
[23:23:00.675650] Epoch: [2] Total time: 0:00:02 (1.1436 s / it)
[23:23:00.680897] Averaged stats: lr: 0.000005  loss_kl: 0.1786 (0.1823)  loss_lp0: 0.0022 (0.0022)  loss_lp1: 0.0020 (0.0020)  loss_lp2: 0.0018 (0.0018)  loss_lp_render: 0.0018 (0.0018)  loss_lp_emd0: 0.0147 (0.0152)  loss_lp_emd2: 0.0106 (0.0111)  loss_lp_emd_render: 0.0107 (0.0109)  loss_render_rgb: 0.0073 (0.0083)  loss_render_depth: 0.1937 (0.2035)  loss_render_mask: 0.1949 (0.2046)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2308 (0.2369)  loss: 0.6714 (0.6983)  psnr: 20.5562 (21.0435)
[23:23:00.683032] Training time 0:00:11
Collecting data...
Generating '/tmp/nsys-report-854c.qdstrm'
[1/1] [0%                          ] vae.nsys-rep[1/1] [0%                          ] vae.nsys-rep[1/1] [11%                         ] vae.nsys-rep[1/1] [==19%                       ] vae.nsys-rep[1/1] [====27%                     ] vae.nsys-rep[1/1] [=======36%                  ] vae.nsys-rep[1/1] [=========44%                ] vae.nsys-rep[1/1] [===========53%              ] vae.nsys-rep[1/1] [==============61%           ] vae.nsys-rep[1/1] [================70%         ] vae.nsys-rep[1/1] [==================76%       ] vae.nsys-rep[1/1] [==================77%       ] vae.nsys-rep[1/1] [========================100%] vae.nsys-rep[1/1] [========================100%] vae.nsys-rep
Generated:
	/home/rb/AtlasGaussians/profiler/vae.nsys-rep
