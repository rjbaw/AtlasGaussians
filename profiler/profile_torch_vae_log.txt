Not using distributed mode
[18:08:13.407261] job dir: /workspace/home/AtlasGaussians
[18:08:13.407280] Namespace(config_path='config/shapenet/train_car_full.yaml',
log_dir='output/vae/shapenet/vae_car_full',
resume='output/vae/shapenet/vae_car_full/ckpt/checkpoint-50_model_only.pth',
start_epoch=0,
eval=False,
infer=False,
dist_eval=False,
device='cuda',
distributed=False,
world_size=1,
local_rank=-1,
dist_url='env://',
output_dir='output/vae/shapenet/vae_car_full/ckpt')
[18:08:13.434430] ['02958343']
[18:08:13.434548] ['02958343']
[18:08:13.439437] Copied .gitignore to output/vae/shapenet/vae_car_full/code/.gitignore
[18:08:13.439489] Copied README.md to output/vae/shapenet/vae_car_full/code/README.md
[18:08:13.439527] Copied config/objaverse/train_18k_base.yaml to output/vae/shapenet/vae_car_full/code/config/objaverse/train_18k_base.yaml
[18:08:13.439561] Copied config/objaverse/train_18k_full.yaml to output/vae/shapenet/vae_car_full/code/config/objaverse/train_18k_full.yaml
[18:08:13.439593] Copied config/shapenet/train_car_base.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_car_base.yaml
[18:08:13.439623] Copied config/shapenet/train_car_full.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_car_full.yaml
[18:08:13.439663] Copied config/shapenet/train_chair_base.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_chair_base.yaml
[18:08:13.439693] Copied config/shapenet/train_chair_full.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_chair_full.yaml
[18:08:13.439722] Copied config/shapenet/train_plane_base.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_plane_base.yaml
[18:08:13.439752] Copied config/shapenet/train_plane_full.yaml to output/vae/shapenet/vae_car_full/code/config/shapenet/train_plane_full.yaml
[18:08:13.439797] Copied datasets/objaverse.py to output/vae/shapenet/vae_car_full/code/datasets/objaverse.py
[18:08:13.439832] Copied datasets/shapenet.py to output/vae/shapenet/vae_car_full/code/datasets/shapenet.py
[18:08:13.439865] Copied datasets/splits/shapenet/02691156_test.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02691156_test.txt
[18:08:13.439895] Copied datasets/splits/shapenet/02691156_train.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02691156_train.txt
[18:08:13.439925] Copied datasets/splits/shapenet/02958343_test.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02958343_test.txt
[18:08:13.439956] Copied datasets/splits/shapenet/02958343_train.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/02958343_train.txt
[18:08:13.439985] Copied datasets/splits/shapenet/03001627_test.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/03001627_test.txt
[18:08:13.440015] Copied datasets/splits/shapenet/03001627_train.txt to output/vae/shapenet/vae_car_full/code/datasets/splits/shapenet/03001627_train.txt
[18:08:13.440058] Copied engine_ae.py to output/vae/shapenet/vae_car_full/code/engine_ae.py
[18:08:13.440090] Copied engine_class_cond.py to output/vae/shapenet/vae_car_full/code/engine_class_cond.py
[18:08:13.440121] Copied evaluations/fid_scores/fid.sh to output/vae/shapenet/vae_car_full/code/evaluations/fid_scores/fid.sh
[18:08:13.440161] Copied evaluations/fid_scores/fid_score.py to output/vae/shapenet/vae_car_full/code/evaluations/fid_scores/fid_score.py
[18:08:13.440190] Copied evaluations/fid_scores/kid.sh to output/vae/shapenet/vae_car_full/code/evaluations/fid_scores/kid.sh
[18:08:13.440231] Copied evaluations/fid_scores/kid_score.py to output/vae/shapenet/vae_car_full/code/evaluations/fid_scores/kid_score.py
[18:08:13.440263] Copied main_ae.py to output/vae/shapenet/vae_car_full/code/main_ae.py
[18:08:13.440304] Copied main_class_cond.py to output/vae/shapenet/vae_car_full/code/main_class_cond.py
[18:08:13.440334] Copied requirements.txt to output/vae/shapenet/vae_car_full/code/requirements.txt
[18:08:13.440375] Copied sample_class_cond_cfg.py to output/vae/shapenet/vae_car_full/code/sample_class_cond_cfg.py
[18:08:13.440407] Copied scripts/ldm/train_dgx.sh to output/vae/shapenet/vae_car_full/code/scripts/ldm/train_dgx.sh
[18:08:13.440437] Copied scripts/vae/train_dgx.sh to output/vae/shapenet/vae_car_full/code/scripts/vae/train_dgx.sh
[18:08:13.440478] Copied util/emd/README.md to output/vae/shapenet/vae_car_full/code/util/emd/README.md
[18:08:13.440521] Copied util/misc.py to output/vae/shapenet/vae_car_full/code/util/misc.py
[18:08:15.487504] Model = KLAutoEncoder(
  (img_encoder): LEAP_Encoder(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x NestedTensorBlock(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (backbone_out): BackboneOutBlock()
  )
  (point_embed): PointFourierEncoder(
    (mlp): Linear(in_features=51, out_features=512, bias=True)
  )
  (pc_encoder): TransformerDecoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (multihead_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
    )
    (linear1): Linear(in_features=512, out_features=2048, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (linear2): Linear(in_features=2048, out_features=512, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
  )
  (embed_layers): ModuleList(
    (0): TransformerDecoderLayerAda(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
    )
  )
  (enc_layers): Sequential(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
    )
  )
  (mean_fc): Linear(in_features=512, out_features=16, bias=True)
  (logvar_fc): Linear(in_features=512, out_features=16, bias=True)
  (proj): Linear(in_features=16, out_features=512, bias=True)
  (latent_net): LatentNet(
    (ca_layer): TransformerDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=2048, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (linear2): Linear(in_features=2048, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
    )
    (layers): ModuleList(
      (0-7): 8 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (up_fc): Linear(in_features=512, out_features=1024, bias=True)
    (up_layers): ModuleList(
      (0-3): 4 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (lp_net): LPNet(
    (point_fourier_encoder): PointFourierEncoder(
      (mlp): Linear(in_features=34, out_features=128, bias=True)
    )
    (lps_net): LPSNet(
      (up_fc): Linear(in_features=256, out_features=512, bias=True)
      (global_fc): Linear(in_features=256, out_features=128, bias=True)
      (anchor_fc): Linear(in_features=256, out_features=128, bias=True)
      (geom_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (attr_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (global_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (anchor_layers): Sequential(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (to_anchor): Sequential(
        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): GELU(approximate='none')
        (3): Linear(in_features=128, out_features=3, bias=True)
      )
    )
    (gs_decoder_geom): GS_Decoder(
      (decoder_cross_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (to_outputs): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): GELU(approximate='none')
      )
      (to_pos): Linear(in_features=128, out_features=3, bias=True)
      (to_opacity): Linear(in_features=128, out_features=1, bias=True)
      (to_scale): Linear(in_features=128, out_features=3, bias=True)
      (to_rotation): Linear(in_features=128, out_features=4, bias=True)
      (to_rgb): Linear(in_features=128, out_features=3, bias=True)
    )
    (gs_decoder_attr): GS_Decoder(
      (decoder_cross_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (to_outputs): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): GELU(approximate='none')
      )
      (to_pos): Linear(in_features=128, out_features=3, bias=True)
      (to_opacity): Linear(in_features=128, out_features=1, bias=True)
      (to_scale): Linear(in_features=128, out_features=3, bias=True)
      (to_rotation): Linear(in_features=128, out_features=4, bias=True)
      (to_rgb): Linear(in_features=128, out_features=3, bias=True)
    )
  )
  (lpips_loss): LPIPS(
    (net): vgg16(
      (slice1): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
      )
      (slice2): Sequential(
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (6): ReLU(inplace=True)
        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (8): ReLU(inplace=True)
      )
      (slice3): Sequential(
        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU(inplace=True)
        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (13): ReLU(inplace=True)
        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (15): ReLU(inplace=True)
      )
      (slice4): Sequential(
        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (18): ReLU(inplace=True)
        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (20): ReLU(inplace=True)
        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (22): ReLU(inplace=True)
      )
      (slice5): Sequential(
        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (25): ReLU(inplace=True)
        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (27): ReLU(inplace=True)
        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (29): ReLU(inplace=True)
      )
    )
    (scaling_layer): ScalingLayer()
    (lin0): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin1): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin2): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin3): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lin4): NetLinLayer(
      (model): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (lins): ModuleList(
      (0): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3-4): 2 x NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
  )
)
[18:08:15.487571] number of params (M): 141.69
[18:08:15.487644] actual lr: 1.00e-04
[18:08:15.487654] accumulate grad iterations: 1
[18:08:15.487656] effective batch size: 4
[18:08:15.488865] criterion = BCEWithLogitsLoss()
[18:08:15.618088] Resume checkpoint output/vae/shapenet/vae_car_full/ckpt/checkpoint-50_model_only.pth
[18:08:15.621976] Start training for 1000 epochs
[18:08:15.632763] log_dir: output/vae/shapenet/vae_car_full
[18:08:18.594496] Epoch: [0]  [0/2]  eta: 0:00:05  lr: 0.000000  loss_kl: 0.1110 (0.1110)  loss_lp0: 0.0018 (0.0018)  loss_lp1: 0.0018 (0.0018)  loss_lp2: 0.0016 (0.0016)  loss_lp_render: 0.0016 (0.0016)  loss_lp_emd0: 0.0403 (0.0403)  loss_lp_emd2: 0.0332 (0.0332)  loss_lp_emd_render: 0.0329 (0.0329)  loss_render_rgb: 0.0078 (0.0078)  loss_render_depth: 0.1114 (0.1114)  loss_render_mask: 0.1173 (0.1173)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2255 (0.2255)  loss: 0.5751 (0.5751)  psnr: 21.1286 (21.1286)  time: 2.9408  data: 0.6681  max mem: 17339
[18:08:19.107268] Epoch: [0]  [1/2]  eta: 0:00:01  lr: 0.000001  loss_kl: 0.1110 (0.1132)  loss_lp0: 0.0016 (0.0017)  loss_lp1: 0.0015 (0.0016)  loss_lp2: 0.0013 (0.0014)  loss_lp_render: 0.0013 (0.0014)  loss_lp_emd0: 0.0342 (0.0373)  loss_lp_emd2: 0.0287 (0.0310)  loss_lp_emd_render: 0.0286 (0.0308)  loss_render_rgb: 0.0076 (0.0077)  loss_render_depth: 0.1086 (0.1100)  loss_render_mask: 0.1145 (0.1159)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2210 (0.2232)  loss: 0.5488 (0.5620)  psnr: 21.1286 (21.1780)  time: 1.7264  data: 0.3341  max mem: 17339
[18:08:19.132190] Epoch: [0] Total time: 0:00:03 (1.7496 s / it)
[18:08:19.135582] Averaged stats: lr: 0.000001  loss_kl: 0.1110 (0.1132)  loss_lp0: 0.0016 (0.0017)  loss_lp1: 0.0015 (0.0016)  loss_lp2: 0.0013 (0.0014)  loss_lp_render: 0.0013 (0.0014)  loss_lp_emd0: 0.0342 (0.0373)  loss_lp_emd2: 0.0287 (0.0310)  loss_lp_emd_render: 0.0286 (0.0308)  loss_render_rgb: 0.0076 (0.0077)  loss_render_depth: 0.1086 (0.1100)  loss_render_mask: 0.1145 (0.1159)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2210 (0.2232)  loss: 0.5488 (0.5620)  psnr: 21.1286 (21.1780)
[18:08:21.898474] Test:  [0/1]  eta: 0:00:01  loss_kl: 0.1725 (0.1725)  loss_lp0: 0.0011 (0.0011)  loss_lp1: 0.0011 (0.0011)  loss_lp2: 0.0010 (0.0010)  loss_lp_render: 0.0010 (0.0010)  loss_lp_emd0: 0.0324 (0.0324)  loss_lp_emd2: 0.0276 (0.0276)  loss_lp_emd_render: 0.0276 (0.0276)  loss_render_rgb: 0.0072 (0.0072)  loss_render_depth: 0.1025 (0.1025)  loss_render_mask: 0.1092 (0.1092)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2203 (0.2203)  loss: 0.5311 (0.5311)  psnr: 21.4349 (21.4349)  time: 1.4197  data: 0.5556  max mem: 17339
[18:08:21.943124] Test: Total time: 0:00:01 (1.4650 s / it)
[18:08:22.002111] log_dir: output/vae/shapenet/vae_car_full
[18:08:24.415187] Epoch: [1]  [0/2]  eta: 0:00:04  lr: 0.000002  loss_kl: 0.1142 (0.1142)  loss_lp0: 0.0018 (0.0018)  loss_lp1: 0.0018 (0.0018)  loss_lp2: 0.0016 (0.0016)  loss_lp_render: 0.0016 (0.0016)  loss_lp_emd0: 0.0407 (0.0407)  loss_lp_emd2: 0.0332 (0.0332)  loss_lp_emd_render: 0.0325 (0.0325)  loss_render_rgb: 0.0073 (0.0073)  loss_render_depth: 0.1069 (0.1069)  loss_render_mask: 0.1130 (0.1130)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2173 (0.2173)  loss: 0.5576 (0.5576)  psnr: 21.4104 (21.4104)  time: 2.4120  data: 1.8498  max mem: 17339
[18:08:24.892647] Epoch: [1]  [1/2]  eta: 0:00:01  lr: 0.000003  loss_kl: 0.1142 (0.1170)  loss_lp0: 0.0014 (0.0016)  loss_lp1: 0.0013 (0.0016)  loss_lp2: 0.0012 (0.0014)  loss_lp_render: 0.0012 (0.0014)  loss_lp_emd0: 0.0320 (0.0364)  loss_lp_emd2: 0.0266 (0.0299)  loss_lp_emd_render: 0.0267 (0.0296)  loss_render_rgb: 0.0073 (0.0074)  loss_render_depth: 0.1069 (0.1088)  loss_render_mask: 0.1130 (0.1154)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2173 (0.2212)  loss: 0.5516 (0.5546)  psnr: 21.2683 (21.3394)  time: 1.4443  data: 0.9257  max mem: 17339
[18:08:24.940801] Epoch: [1] Total time: 0:00:02 (1.4693 s / it)
[18:08:24.945076] Averaged stats: lr: 0.000003  loss_kl: 0.1142 (0.1170)  loss_lp0: 0.0014 (0.0016)  loss_lp1: 0.0013 (0.0016)  loss_lp2: 0.0012 (0.0014)  loss_lp_render: 0.0012 (0.0014)  loss_lp_emd0: 0.0320 (0.0364)  loss_lp_emd2: 0.0266 (0.0299)  loss_lp_emd_render: 0.0267 (0.0296)  loss_render_rgb: 0.0073 (0.0074)  loss_render_depth: 0.1069 (0.1088)  loss_render_mask: 0.1130 (0.1154)  loss_scale_std: 0.0000 (0.0000)  loss_expand: 0.0003 (0.0003)  loss_lpips: 0.2173 (0.2212)  loss: 0.5516 (0.5546)  psnr: 21.2683 (21.3394)
[18:08:42.505697] -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                          train_epoch_0         0.00%       0.000us         0.00%       0.000us       0.000us        5.510s       178.72%        5.510s        1.102s           0 b           0 b           0 b           0 b             5  
                                          train_epoch_0        11.74%        1.022s        40.29%        3.508s        3.508s       0.000us         0.00%        2.316s        2.316s       2.59 Kb    -230.76 Mb       1.09 Gb     -29.15 Gb             1  
                                           eval_epoch_0         0.00%       0.000us         0.00%       0.000us       0.000us        2.204s        71.50%        2.204s     440.862ms           0 b           0 b           0 b           0 b             5  
                                           aten::conv2d         0.00%     296.511us        17.23%        1.501s       9.095ms       0.000us         0.00%        1.841s      11.160ms           0 b           0 b       9.59 Gb      -2.85 Mb           165  
                                      aten::convolution         0.01%     482.297us        16.12%        1.404s       8.773ms       0.000us         0.00%        1.836s      11.478ms           0 b           0 b       9.54 Gb           0 b           160  
                                     aten::_convolution         0.01%     831.262us        16.12%        1.403s       8.770ms       0.000us         0.00%        1.836s      11.478ms           0 b           0 b       9.54 Gb      -9.00 Mb           160  
                                aten::cudnn_convolution         0.71%      61.508ms        16.02%        1.395s       8.719ms     732.819ms        23.77%        1.826s      11.410ms           0 b           0 b       9.55 Gb       9.27 Gb           160  
                                            emdFunction         0.02%       1.880ms         0.16%      14.013ms     934.174us        1.447s        46.95%        1.448s      96.507ms           0 b           0 b       5.91 Mb     -26.67 Mb            15  
Bid(int, int, float const*, float const*, float, int...         0.00%       0.000us         0.00%       0.000us       0.000us        1.440s        46.69%        1.440s       1.919ms           0 b           0 b           0 b           0 b           750  
                                          train_epoch_1         0.00%       0.000us         0.00%       0.000us       0.000us        1.039s        33.70%        1.039s        1.039s           0 b           0 b           0 b           0 b             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 8.708s
Self CUDA time total: 3.083s

[18:08:42.505722] Training time 0:00:26
